{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoaaRgwShOEU",
        "outputId": "cc60e535-87c5-45f5-f752-a24b0040efbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mWJsW65FhaA0"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/MyDrive/T5-Sentinel-public'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caSFFwd2hhN8",
        "outputId": "ad728ad8-0842-47c1-e321-d6a4c87997eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.gitignore', 'README.md', 'LICENSE', 'memoizer', 'cache', 'result', 'evaluator', 'pipeline', '.git', 'generator', 'data', 'detector', 'requirements.txt', 'temp_result.txt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "files = os.listdir(folder_path)\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoXw0rtPhnTZ",
        "outputId": "11c39bed-477c-41d3-f2b4-cd42d0299381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/T5-Sentinel-public\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/T5-Sentinel-public\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PJsNNjSEhrvF",
        "outputId": "add83d17-a515-4a57-c2d5-4456a38b56ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/T5-Sentinel-public'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXkvq0aMhthf",
        "outputId": "350d99bb-85da-4559-bdaf-11ac149f5da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aiohttp==3.9.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.9.1)\n",
            "Requirement already satisfied: bardapi==0.1.38 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.38)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (8.1.7)\n",
            "Requirement already satisfied: matplotlib==3.7.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.4)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.24.4)\n",
            "Requirement already satisfied: openai==1.6.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: protobuf==4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.25.1)\n",
            "Requirement already satisfied: pydantic==2.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.5.3)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (6.0.1)\n",
            "Requirement already satisfied: Requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.31.0)\n",
            "Requirement already satisfied: scikit_learn==1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.3.2)\n",
            "Requirement already satisfied: seaborn==0.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.13.1)\n",
            "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2.1.2)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (4.66.1)\n",
            "Requirement already satisfied: transformers==4.36.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (4.36.2)\n",
            "Requirement already satisfied: Unidecode==1.3.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (1.3.7)\n",
            "Requirement already satisfied: vertexai==0.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (0.0.1)\n",
            "Requirement already satisfied: wandb==0.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.16.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->-r requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: httpx[http2]>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from bardapi==0.1.38->-r requirements.txt (line 2)) (0.27.0)\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.10/dist-packages (from bardapi==0.1.38->-r requirements.txt (line 2)) (1.11.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from bardapi==0.1.38->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: google-cloud-translate in /usr/local/lib/python3.10/dist-packages (from bardapi==0.1.38->-r requirements.txt (line 2)) (3.11.3)\n",
            "Requirement already satisfied: browser-cookie3 in /usr/local/lib/python3.10/dist-packages (from bardapi==0.1.38->-r requirements.txt (line 2)) (0.19.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from bardapi==0.1.38->-r requirements.txt (line 2)) (1.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 4)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 4)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 4)) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 4)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.4->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (2023.12.25)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1->-r requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.6.1->-r requirements.txt (line 7)) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1->-r requirements.txt (line 7)) (4.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.5.3->-r requirements.txt (line 9)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.5.3->-r requirements.txt (line 9)) (2.14.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 11)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 11)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 11)) (2024.2.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.3.2->-r requirements.txt (line 12)) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.3.2->-r requirements.txt (line 12)) (3.4.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.13.1->-r requirements.txt (line 13)) (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->-r requirements.txt (line 14)) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2->-r requirements.txt (line 16)) (0.20.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2->-r requirements.txt (line 16)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2->-r requirements.txt (line 16)) (0.4.2)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.1->-r requirements.txt (line 19)) (3.1.43)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.1->-r requirements.txt (line 19)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.1->-r requirements.txt (line 19)) (1.45.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.1->-r requirements.txt (line 19)) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.1->-r requirements.txt (line 19)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.1->-r requirements.txt (line 19)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.1->-r requirements.txt (line 19)) (1.4.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->-r requirements.txt (line 14)) (12.4.127)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.6.1->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.16.1->-r requirements.txt (line 19)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.16.1->-r requirements.txt (line 19)) (4.0.11)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->bardapi==0.1.38->-r requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[http2]>=0.20.0->bardapi==0.1.38->-r requirements.txt (line 2)) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->bardapi==0.1.38->-r requirements.txt (line 2)) (4.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn==0.13.1->-r requirements.txt (line 13)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn==0.13.1->-r requirements.txt (line 13)) (2024.1)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from browser-cookie3->bardapi==0.1.38->-r requirements.txt (line 2)) (4.3.3)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.10/dist-packages (from browser-cookie3->bardapi==0.1.38->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: jeepney in /usr/lib/python3/dist-packages (from browser-cookie3->bardapi==0.1.38->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator->bardapi==0.1.38->-r requirements.txt (line 2)) (4.12.3)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (2.3.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (1.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->-r requirements.txt (line 14)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->-r requirements.txt (line 14)) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator->bardapi==0.1.38->-r requirements.txt (line 2)) (2.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.16.1->-r requirements.txt (line 19)) (5.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (2.27.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (1.62.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (1.48.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->bardapi==0.1.38->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->bardapi==0.1.38->-r requirements.txt (line 2)) (4.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi==0.1.38->-r requirements.txt (line 2)) (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijhIN5CNi1PV",
        "outputId": "ad5a353c-9ac0-4e84-85ab-1fd0a148ddfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PYTHONPATH: /env/python:/content/drive/MyDrive/T5-Sentinel-public:/content/drive/MyDrive/T5-Sentinel-public\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set PYTHONPATH to include the project directory\n",
        "project_path = \"/content/drive/MyDrive/T5-Sentinel-public\"\n",
        "os.environ['PYTHONPATH'] += f\":{project_path}\"\n",
        "\n",
        "# Optional: Print PYTHONPATH to verify the change\n",
        "print(\"PYTHONPATH:\", os.environ['PYTHONPATH'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bm2U8RAi9oq",
        "outputId": "bb830c75-a4ca-46af-b23d-7306ec81bfc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing task:  calc_t5_full_statistics\n",
            "Cache Miss / Eviction since argument does not match\n",
            "config.json: 100% 1.21k/1.21k [00:00<00:00, 6.24MB/s]\n",
            "model.safetensors: 100% 242M/242M [00:00<00:00, 247MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 546kB/s]\n",
            "tokenizer_config.json: 100% 2.32k/2.32k [00:00<00:00, 11.2MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 3.38MB/s]\n",
            "tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 4.69MB/s]\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:51<00:00, 42.93it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 1208000.22it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:29<00:00, 49.52it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1300429.70it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:28<00:00, 49.76it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1261034.80it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:14<00:00, 48.84it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1173507.22it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:44<00:00, 44.81it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1067843.45it/s]\n",
            "openweb v. rest statistics\n",
            "TPR: 0.832,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.168 \n",
            "Acc: 0.956\n",
            "F1 : 0.886\n",
            "Recall: 0.832,\tPrecision: 0.946\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "chatgpt v. rest statistics\n",
            "TPR: 0.946,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.054 \n",
            "Acc: 0.979\n",
            "F1 : 0.949\n",
            "Recall: 0.946,\tPrecision: 0.952\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "palm v. rest statistics\n",
            "TPR: 0.949,\tTNR: 0.96, FPR: 0.04,\tFNR: 0.051 \n",
            "Acc: 0.957\n",
            "F1 : 0.901\n",
            "Recall: 0.949,\tPrecision: 0.858\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "llama v. rest statistics\n",
            "TPR: 0.942,\tTNR: 0.999, FPR: 0.001,\tFNR: 0.058 \n",
            "Acc: 0.989\n",
            "F1 : 0.969\n",
            "Recall: 0.942,\tPrecision: 0.997\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "gpt2_xl v. rest statistics\n",
            "TPR: 0.983,\tTNR: 0.981, FPR: 0.019,\tFNR: 0.017 \n",
            "Acc: 0.981\n",
            "F1 : 0.955\n",
            "Recall: 0.983,\tPrecision: 0.929\n",
            "\n",
            "Overall average score:\n",
            "F1: 0.932\n",
            "Recall: 0.93,\tPrecision: 0.937\n",
            "\n",
            "\n",
            "Executing task:  calc_t5_hidden_statistics\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5Hidden >> T5HiddenPredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb324607400>] -->\n",
            "Mapping: 100% 7367/7367 [02:47<00:00, 43.95it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 744062.74it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5Hidden >> T5HiddenPredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb324607400>] -->\n",
            "Mapping: 100% 7385/7385 [02:32<00:00, 48.40it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1257252.71it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5Hidden >> T5HiddenPredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb324607400>] -->\n",
            "Mapping: 100% 7400/7400 [02:39<00:00, 46.50it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1178354.20it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5Hidden >> T5HiddenPredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb324607400>] -->\n",
            "Mapping: 100% 6587/6587 [02:21<00:00, 46.45it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 997468.43it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> ExecuteT5Hidden >> T5HiddenPredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb324607400>] -->\n",
            "Mapping: 100% 7385/7385 [02:47<00:00, 44.02it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1069391.85it/s]\n",
            "openweb v. rest statistics\n",
            "TPR: 0.849,\tTNR: 0.906, FPR: 0.094,\tFNR: 0.151 \n",
            "Acc: 0.894\n",
            "F1 : 0.766\n",
            "Recall: 0.849,\tPrecision: 0.698\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "chatgpt v. rest statistics\n",
            "TPR: 0.942,\tTNR: 0.989, FPR: 0.011,\tFNR: 0.058 \n",
            "Acc: 0.98\n",
            "F1 : 0.95\n",
            "Recall: 0.942,\tPrecision: 0.958\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "palm v. rest statistics\n",
            "TPR: 0.954,\tTNR: 0.945, FPR: 0.055,\tFNR: 0.046 \n",
            "Acc: 0.947\n",
            "F1 : 0.881\n",
            "Recall: 0.954,\tPrecision: 0.818\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "llama v. rest statistics\n",
            "TPR: 0.445,\tTNR: 1.0, FPR: 0.0,\tFNR: 0.555 \n",
            "Acc: 0.899\n",
            "F1 : 0.616\n",
            "Recall: 0.445,\tPrecision: 1.0\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "gpt2_xl v. rest statistics\n",
            "TPR: 0.981,\tTNR: 0.966, FPR: 0.034,\tFNR: 0.019 \n",
            "Acc: 0.969\n",
            "F1 : 0.929\n",
            "Recall: 0.981,\tPrecision: 0.881\n",
            "\n",
            "Overall average score:\n",
            "F1: 0.828\n",
            "Recall: 0.834,\tPrecision: 0.871\n",
            "\n",
            "\n",
            "Executing task:  calc_openai_baseline_statistics\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/openai_classifier_output/open-gpt-text.jsonl] --[Map LoadOpenAIPredictionResult >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261432560>] -->\n",
            "Mapping: 100% 7505/7505 [00:00<00:00, 48844.91it/s]\n",
            "Reducing: 100% 7505/7505 [00:00<00:00, 1095543.50it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/openai_classifier_output/open-llama-text.jsonl] --[Map LoadOpenAIPredictionResult >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261432560>] -->\n",
            "Mapping: 100% 6587/6587 [00:00<00:00, 47508.95it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1050010.66it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/openai_classifier_output/open-palm-text.jsonl] --[Map LoadOpenAIPredictionResult >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261432560>] -->\n",
            "Mapping: 100% 7320/7320 [00:00<00:00, 48596.59it/s]\n",
            "Reducing: 100% 7320/7320 [00:00<00:00, 1042912.64it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/openai_classifier_output/open-web-text.jsonl] --[Map LoadOpenAIPredictionResult >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261432560>] -->\n",
            "Mapping: 100% 7367/7367 [00:00<00:00, 86927.05it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 1664930.09it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/openai_classifier_output/gpt2-output.jsonl] --[Map LoadOpenAIPredictionResult >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261432560>] -->\n",
            "Mapping: 100% 7385/7385 [00:00<00:00, 81414.65it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1673325.86it/s]\n",
            "OpenAI classifier human-to-generated statistics\n",
            "TPR: 0.985,\tTNR: 0.293, FPR: 0.707,\tFNR: 0.015 \n",
            "Acc: 0.434\n",
            "F1 : 0.415\n",
            "Recall: 0.985,\tPrecision: 0.263\n",
            "AUROC: 0.795\n",
            "\n",
            "\n",
            "Executing task:  calc_zerogpt_baseline_statistics\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/zerogpt_classifier_output/open-gpt-text.jsonl] --[Map LoadZeroGPTPrediction >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261433880>] -->\n",
            "Mapping: 100% 7264/7264 [00:00<00:00, 96038.73it/s]\n",
            "Reducing: 100% 7264/7264 [00:00<00:00, 1613826.17it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/zerogpt_classifier_output/open-llama-text.jsonl] --[Map LoadZeroGPTPrediction >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261433880>] -->\n",
            "Mapping: 100% 6546/6546 [00:00<00:00, 35719.24it/s]\n",
            "Reducing: 100% 6546/6546 [00:00<00:00, 1458016.78it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/zerogpt_classifier_output/open-palm-text.jsonl] --[Map LoadZeroGPTPrediction >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261433880>] -->\n",
            "Mapping: 100% 7394/7394 [00:00<00:00, 87755.69it/s]\n",
            "Reducing: 100% 7394/7394 [00:00<00:00, 1666005.04it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/zerogpt_classifier_output/open-web-text.jsonl] --[Map LoadZeroGPTPrediction >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261433880>] -->\n",
            "Mapping: 100% 4391/4391 [00:00<00:00, 85749.09it/s]\n",
            "Reducing: 100% 4391/4391 [00:00<00:00, 1268576.17it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "You are using more multiprocess worker than cpu count (2)!\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/baselines/zerogpt_classifier_output/gpt2-output.jsonl] --[Map LoadZeroGPTPrediction >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb261433880>] -->\n",
            "Mapping: 100% 7331/7331 [00:00<00:00, 49474.88it/s]\n",
            "Reducing: 100% 7331/7331 [00:00<00:00, 1337005.07it/s]\n",
            "ZeroGPT classifier human-to-generated statistics\n",
            "TPR: 0.839,\tTNR: 0.259, FPR: 0.741,\tFNR: 0.161 \n",
            "Acc: 0.336\n",
            "F1 : 0.252\n",
            "Recall: 0.839,\tPrecision: 0.148\n",
            "AUROC: 0.533\n",
            "\n",
            "\n",
            "Executing task:  implement\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "Ablation Filename: test-dirty.jsonl\n",
            "TPR: 0.832,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.168 \n",
            "Acc: 0.956\n",
            "F1 : 0.886\n",
            "Recall: 0.832,\tPrecision: 0.946\n",
            "AUROC: 0.965\n",
            "\n",
            "\n",
            "Executing task:  implement\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test.variant1.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14734/14734 [05:41<00:00, 43.14it/s]\n",
            "Reducing: 100% 14734/14734 [00:00<00:00, 719578.90it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test.variant1.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14770/14770 [04:58<00:00, 49.51it/s]\n",
            "Reducing: 100% 14770/14770 [00:00<00:00, 767456.67it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test.variant1.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14800/14800 [04:58<00:00, 49.59it/s]\n",
            "Reducing: 100% 14800/14800 [00:00<00:00, 1133885.57it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test.variant1.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 13174/13174 [04:32<00:00, 48.34it/s]\n",
            "Reducing: 100% 13174/13174 [00:00<00:00, 1107751.67it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test.variant1.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14770/14770 [05:32<00:00, 44.38it/s]\n",
            "Reducing: 100% 14770/14770 [00:00<00:00, 975081.77it/s]\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "Ablation Filename: test.variant1.jsonl\n",
            "TPR: 0.832,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.168 \n",
            "Acc: 0.956\n",
            "F1 : 0.886\n",
            "Recall: 0.832,\tPrecision: 0.946\n",
            "AUROC: 0.965\n",
            "\n",
            "\n",
            "Executing task:  implement\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test.variant2.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14734/14734 [05:41<00:00, 43.10it/s]\n",
            "Reducing: 100% 14734/14734 [00:00<00:00, 1073661.38it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test.variant2.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14770/14770 [04:59<00:00, 49.34it/s]\n",
            "Reducing: 100% 14770/14770 [00:00<00:00, 1152813.09it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test.variant2.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14800/14800 [04:58<00:00, 49.62it/s]\n",
            "Reducing: 100% 14800/14800 [00:00<00:00, 1163468.52it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test.variant2.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 13174/13174 [04:31<00:00, 48.53it/s]\n",
            "Reducing: 100% 13174/13174 [00:00<00:00, 1152963.19it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test.variant2.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14770/14770 [05:30<00:00, 44.63it/s]\n",
            "Reducing: 100% 14770/14770 [00:00<00:00, 1049037.66it/s]\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "Ablation Filename: test.variant2.jsonl\n",
            "TPR: 0.752,\tTNR: 0.986, FPR: 0.014,\tFNR: 0.248 \n",
            "Acc: 0.938\n",
            "F1 : 0.832\n",
            "Recall: 0.752,\tPrecision: 0.931\n",
            "AUROC: 0.947\n",
            "\n",
            "\n",
            "Executing task:  implement\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test.variant3.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14734/14734 [05:34<00:00, 44.10it/s]\n",
            "Reducing: 100% 14734/14734 [00:00<00:00, 1010099.13it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test.variant3.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14770/14770 [04:58<00:00, 49.41it/s]\n",
            "Reducing: 100% 14770/14770 [00:00<00:00, 1153800.75it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test.variant3.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14800/14800 [04:51<00:00, 50.75it/s]\n",
            "Reducing: 100% 14800/14800 [00:00<00:00, 1152024.70it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test.variant3.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 13174/13174 [04:27<00:00, 49.20it/s]\n",
            "Reducing: 100% 13174/13174 [00:00<00:00, 1082597.20it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test.variant3.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14770/14770 [05:24<00:00, 45.50it/s]\n",
            "Reducing: 100% 14770/14770 [00:00<00:00, 1071834.15it/s]\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "Ablation Filename: test.variant3.jsonl\n",
            "TPR: 0.801,\tTNR: 0.628, FPR: 0.372,\tFNR: 0.199 \n",
            "Acc: 0.664\n",
            "F1 : 0.493\n",
            "Recall: 0.801,\tPrecision: 0.356\n",
            "AUROC: 0.775\n",
            "\n",
            "\n",
            "Executing task:  implement\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test.variant4.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14734/14734 [05:37<00:00, 43.66it/s]\n",
            "Reducing: 100% 14734/14734 [00:00<00:00, 1085276.07it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test.variant4.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14770/14770 [04:59<00:00, 49.37it/s]\n",
            "Reducing: 100% 14770/14770 [00:00<00:00, 1161480.21it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test.variant4.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14800/14800 [04:59<00:00, 49.34it/s]\n",
            "Reducing: 100% 14800/14800 [00:00<00:00, 1155563.19it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test.variant4.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 13174/13174 [04:32<00:00, 48.34it/s]\n",
            "Reducing: 100% 13174/13174 [00:00<00:00, 1003974.80it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test.variant4.jsonl] --[Map FromJsonStr >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 14770/14770 [05:31<00:00, 44.49it/s]\n",
            "Reducing: 100% 14770/14770 [00:00<00:00, 823374.45it/s]\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "Ablation Filename: test.variant4.jsonl\n",
            "TPR: 0.885,\tTNR: 0.957, FPR: 0.043,\tFNR: 0.115 \n",
            "Acc: 0.943\n",
            "F1 : 0.863\n",
            "Recall: 0.885,\tPrecision: 0.842\n",
            "AUROC: 0.966\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "openweb-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.849,\tTNR: 0.906, FPR: 0.094,\tFNR: 0.151 \n",
            "Acc: 0.894\n",
            "F1 : 0.766\n",
            "Recall: 0.849,\tPrecision: 0.698\n",
            "AUROC: 0.93\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "chatgpt-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.942,\tTNR: 0.989, FPR: 0.011,\tFNR: 0.058 \n",
            "Acc: 0.98\n",
            "F1 : 0.95\n",
            "Recall: 0.942,\tPrecision: 0.958\n",
            "AUROC: 0.985\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "palm-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.954,\tTNR: 0.945, FPR: 0.055,\tFNR: 0.046 \n",
            "Acc: 0.947\n",
            "F1 : 0.881\n",
            "Recall: 0.954,\tPrecision: 0.818\n",
            "AUROC: 0.98\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "llama-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.445,\tTNR: 1.0, FPR: 0.0,\tFNR: 0.555 \n",
            "Acc: 0.899\n",
            "F1 : 0.616\n",
            "Recall: 0.445,\tPrecision: 1.0\n",
            "AUROC: 0.788\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "gpt2_xl-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.981,\tTNR: 0.966, FPR: 0.034,\tFNR: 0.019 \n",
            "Acc: 0.969\n",
            "F1 : 0.929\n",
            "Recall: 0.981,\tPrecision: 0.881\n",
            "AUROC: 0.989\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/openai_prediction.pt\n",
            "Reusing existing cache from cache/openai_prediction.pt\n",
            "OpenAI classifier human-to-generated statistics\n",
            "data/baselines/openai_classifier_output/open-gpt-text.jsonl\n",
            "TPR: 0.985,\tTNR: 0.161, FPR: 0.839,\tFNR: 0.015 \n",
            "Acc: 0.569\n",
            "F1 : 0.694\n",
            "Recall: 0.985,\tPrecision: 0.536\n",
            "AUROC: 0.761\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/openai_prediction.pt\n",
            "Reusing existing cache from cache/openai_prediction.pt\n",
            "OpenAI classifier human-to-generated statistics\n",
            "data/baselines/openai_classifier_output/open-palm-text.jsonl\n",
            "TPR: 0.985,\tTNR: 0.331, FPR: 0.669,\tFNR: 0.015 \n",
            "Acc: 0.659\n",
            "F1 : 0.743\n",
            "Recall: 0.985,\tPrecision: 0.597\n",
            "AUROC: 0.829\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/openai_prediction.pt\n",
            "Reusing existing cache from cache/openai_prediction.pt\n",
            "OpenAI classifier human-to-generated statistics\n",
            "data/baselines/openai_classifier_output/open-llama-text.jsonl\n",
            "TPR: 0.985,\tTNR: 0.113, FPR: 0.887,\tFNR: 0.015 \n",
            "Acc: 0.573\n",
            "F1 : 0.709\n",
            "Recall: 0.985,\tPrecision: 0.554\n",
            "AUROC: 0.676\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/openai_prediction.pt\n",
            "Reusing existing cache from cache/openai_prediction.pt\n",
            "OpenAI classifier human-to-generated statistics\n",
            "data/baselines/openai_classifier_output/gpt2-output.jsonl\n",
            "TPR: 0.985,\tTNR: 0.551, FPR: 0.449,\tFNR: 0.015 \n",
            "Acc: 0.768\n",
            "F1 : 0.809\n",
            "Recall: 0.985,\tPrecision: 0.687\n",
            "AUROC: 0.901\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/zerogpt_prediction.pt\n",
            "Reusing existing cache from cache/zerogpt_prediction.pt\n",
            "ZeroGPT classifier human-to-generated statistics\n",
            "data/baselines/zerogpt_classifier_output/open-gpt-text.jsonl\n",
            "TPR: 0.839,\tTNR: 0.284, FPR: 0.716,\tFNR: 0.161 \n",
            "Acc: 0.493\n",
            "F1 : 0.555\n",
            "Recall: 0.839,\tPrecision: 0.414\n",
            "AUROC: 0.576\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/zerogpt_prediction.pt\n",
            "Reusing existing cache from cache/zerogpt_prediction.pt\n",
            "ZeroGPT classifier human-to-generated statistics\n",
            "data/baselines/zerogpt_classifier_output/open-palm-text.jsonl\n",
            "TPR: 0.839,\tTNR: 0.557, FPR: 0.443,\tFNR: 0.161 \n",
            "Acc: 0.662\n",
            "F1 : 0.649\n",
            "Recall: 0.839,\tPrecision: 0.529\n",
            "AUROC: 0.735\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/zerogpt_prediction.pt\n",
            "Reusing existing cache from cache/zerogpt_prediction.pt\n",
            "ZeroGPT classifier human-to-generated statistics\n",
            "data/baselines/zerogpt_classifier_output/open-llama-text.jsonl\n",
            "TPR: 0.839,\tTNR: 0.064, FPR: 0.936,\tFNR: 0.161 \n",
            "Acc: 0.375\n",
            "F1 : 0.519\n",
            "Recall: 0.839,\tPrecision: 0.375\n",
            "AUROC: 0.367\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/zerogpt_prediction.pt\n",
            "Reusing existing cache from cache/zerogpt_prediction.pt\n",
            "ZeroGPT classifier human-to-generated statistics\n",
            "data/baselines/zerogpt_classifier_output/gpt2-output.jsonl\n",
            "TPR: 0.839,\tTNR: 0.109, FPR: 0.891,\tFNR: 0.161 \n",
            "Acc: 0.382\n",
            "F1 : 0.504\n",
            "Recall: 0.839,\tPrecision: 0.36\n",
            "AUROC: 0.435\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "chatgpt-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.946,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.054 \n",
            "Acc: 0.979\n",
            "F1 : 0.949\n",
            "Recall: 0.946,\tPrecision: 0.952\n",
            "AUROC: 0.989\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "palm-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.949,\tTNR: 0.96, FPR: 0.04,\tFNR: 0.051 \n",
            "Acc: 0.957\n",
            "F1 : 0.901\n",
            "Recall: 0.949,\tPrecision: 0.858\n",
            "AUROC: 0.984\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "llama-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.942,\tTNR: 0.999, FPR: 0.001,\tFNR: 0.058 \n",
            "Acc: 0.989\n",
            "F1 : 0.969\n",
            "Recall: 0.942,\tPrecision: 0.997\n",
            "AUROC: 0.989\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "gpt2_xl-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.983,\tTNR: 0.981, FPR: 0.019,\tFNR: 0.017 \n",
            "Acc: 0.981\n",
            "F1 : 0.955\n",
            "Recall: 0.983,\tPrecision: 0.929\n",
            "AUROC: 0.995\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "data/split/open-gpt-text/test-dirty.jsonl\n",
            "TPR: 0.849,\tTNR: 0.995, FPR: 0.005,\tFNR: 0.151 \n",
            "Acc: 0.922\n",
            "F1 : 0.916\n",
            "Recall: 0.849,\tPrecision: 0.994\n",
            "AUROC: 0.971\n",
            "LaTeX:\n",
            ".971 & .916\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "data/split/open-palm-text/test-dirty.jsonl\n",
            "TPR: 0.849,\tTNR: 0.979, FPR: 0.021,\tFNR: 0.151 \n",
            "Acc: 0.914\n",
            "F1 : 0.908\n",
            "Recall: 0.849,\tPrecision: 0.976\n",
            "AUROC: 0.964\n",
            "LaTeX:\n",
            ".964 & .908\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "data/split/open-llama-text/test-dirty.jsonl\n",
            "TPR: 0.849,\tTNR: 0.631, FPR: 0.369,\tFNR: 0.151 \n",
            "Acc: 0.746\n",
            "F1 : 0.779\n",
            "Recall: 0.849,\tPrecision: 0.72\n",
            "AUROC: 0.806\n",
            "LaTeX:\n",
            ".806 & .779\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "Reusing existing cache from cache/t5_hidden_state_predicts.pt\n",
            "T5-Sentinel (ours) classifier human-to-generated statistics\n",
            "data/split/gpt2-output/test-dirty.jsonl\n",
            "TPR: 0.849,\tTNR: 0.989, FPR: 0.011,\tFNR: 0.151 \n",
            "Acc: 0.919\n",
            "F1 : 0.913\n",
            "Recall: 0.849,\tPrecision: 0.988\n",
            "AUROC: 0.965\n",
            "LaTeX:\n",
            ".965 & .913\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "openweb-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.832,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.168 \n",
            "Acc: 0.956\n",
            "F1 : 0.886\n",
            "Recall: 0.832,\tPrecision: 0.946\n",
            "AUROC: 0.965\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "chatgpt-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.946,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.054 \n",
            "Acc: 0.979\n",
            "F1 : 0.949\n",
            "Recall: 0.946,\tPrecision: 0.952\n",
            "AUROC: 0.989\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "palm-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.949,\tTNR: 0.96, FPR: 0.04,\tFNR: 0.051 \n",
            "Acc: 0.957\n",
            "F1 : 0.901\n",
            "Recall: 0.949,\tPrecision: 0.858\n",
            "AUROC: 0.984\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "llama-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.942,\tTNR: 0.999, FPR: 0.001,\tFNR: 0.058 \n",
            "Acc: 0.989\n",
            "F1 : 0.969\n",
            "Recall: 0.942,\tPrecision: 0.997\n",
            "AUROC: 0.989\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "gpt2_xl-to-rest @ test-dirty.jsonl\n",
            "TPR: 0.983,\tTNR: 0.981, FPR: 0.019,\tFNR: 0.017 \n",
            "Acc: 0.981\n",
            "F1 : 0.955\n",
            "Recall: 0.983,\tPrecision: 0.929\n",
            "AUROC: 0.995\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "openweb-to-rest @ test.variant1.jsonl\n",
            "TPR: 0.832,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.168 \n",
            "Acc: 0.956\n",
            "F1 : 0.886\n",
            "Recall: 0.832,\tPrecision: 0.946\n",
            "AUROC: 0.965\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "chatgpt-to-rest @ test.variant1.jsonl\n",
            "TPR: 0.946,\tTNR: 0.988, FPR: 0.012,\tFNR: 0.054 \n",
            "Acc: 0.979\n",
            "F1 : 0.949\n",
            "Recall: 0.946,\tPrecision: 0.952\n",
            "AUROC: 0.989\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "palm-to-rest @ test.variant1.jsonl\n",
            "TPR: 0.949,\tTNR: 0.96, FPR: 0.04,\tFNR: 0.051 \n",
            "Acc: 0.957\n",
            "F1 : 0.901\n",
            "Recall: 0.949,\tPrecision: 0.858\n",
            "AUROC: 0.984\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "llama-to-rest @ test.variant1.jsonl\n",
            "TPR: 0.942,\tTNR: 0.999, FPR: 0.001,\tFNR: 0.058 \n",
            "Acc: 0.989\n",
            "F1 : 0.969\n",
            "Recall: 0.942,\tPrecision: 0.997\n",
            "AUROC: 0.989\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "gpt2_xl-to-rest @ test.variant1.jsonl\n",
            "TPR: 0.983,\tTNR: 0.981, FPR: 0.019,\tFNR: 0.017 \n",
            "Acc: 0.981\n",
            "F1 : 0.955\n",
            "Recall: 0.983,\tPrecision: 0.929\n",
            "AUROC: 0.995\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "openweb-to-rest @ test.variant2.jsonl\n",
            "TPR: 0.752,\tTNR: 0.986, FPR: 0.014,\tFNR: 0.248 \n",
            "Acc: 0.938\n",
            "F1 : 0.832\n",
            "Recall: 0.752,\tPrecision: 0.931\n",
            "AUROC: 0.947\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "chatgpt-to-rest @ test.variant2.jsonl\n",
            "TPR: 0.946,\tTNR: 0.983, FPR: 0.017,\tFNR: 0.054 \n",
            "Acc: 0.976\n",
            "F1 : 0.941\n",
            "Recall: 0.946,\tPrecision: 0.935\n",
            "AUROC: 0.987\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "palm-to-rest @ test.variant2.jsonl\n",
            "TPR: 0.944,\tTNR: 0.957, FPR: 0.043,\tFNR: 0.056 \n",
            "Acc: 0.954\n",
            "F1 : 0.895\n",
            "Recall: 0.944,\tPrecision: 0.85\n",
            "AUROC: 0.983\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "llama-to-rest @ test.variant2.jsonl\n",
            "TPR: 0.898,\tTNR: 1.0, FPR: 0.0,\tFNR: 0.102 \n",
            "Acc: 0.981\n",
            "F1 : 0.946\n",
            "Recall: 0.898,\tPrecision: 0.998\n",
            "AUROC: 0.981\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "gpt2_xl-to-rest @ test.variant2.jsonl\n",
            "TPR: 0.97,\tTNR: 0.957, FPR: 0.043,\tFNR: 0.03 \n",
            "Acc: 0.959\n",
            "F1 : 0.907\n",
            "Recall: 0.97,\tPrecision: 0.852\n",
            "AUROC: 0.988\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "openweb-to-rest @ test.variant3.jsonl\n",
            "TPR: 0.801,\tTNR: 0.628, FPR: 0.372,\tFNR: 0.199 \n",
            "Acc: 0.664\n",
            "F1 : 0.493\n",
            "Recall: 0.801,\tPrecision: 0.356\n",
            "AUROC: 0.775\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "chatgpt-to-rest @ test.variant3.jsonl\n",
            "TPR: 0.051,\tTNR: 0.999, FPR: 0.001,\tFNR: 0.949 \n",
            "Acc: 0.805\n",
            "F1 : 0.096\n",
            "Recall: 0.051,\tPrecision: 0.935\n",
            "AUROC: 0.59\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "palm-to-rest @ test.variant3.jsonl\n",
            "TPR: 0.064,\tTNR: 0.999, FPR: 0.001,\tFNR: 0.936 \n",
            "Acc: 0.807\n",
            "F1 : 0.12\n",
            "Recall: 0.064,\tPrecision: 0.917\n",
            "AUROC: 0.679\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "llama-to-rest @ test.variant3.jsonl\n",
            "TPR: 0.906,\tTNR: 0.966, FPR: 0.034,\tFNR: 0.094 \n",
            "Acc: 0.955\n",
            "F1 : 0.88\n",
            "Recall: 0.906,\tPrecision: 0.855\n",
            "AUROC: 0.974\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "gpt2_xl-to-rest @ test.variant3.jsonl\n",
            "TPR: 0.923,\tTNR: 0.844, FPR: 0.156,\tFNR: 0.077 \n",
            "Acc: 0.86\n",
            "F1 : 0.729\n",
            "Recall: 0.923,\tPrecision: 0.602\n",
            "AUROC: 0.942\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:50<00:00, 43.12it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 801354.74it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:27<00:00, 50.06it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1150714.58it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:29<00:00, 49.65it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1170974.48it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:15<00:00, 48.59it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 997036.47it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:44<00:00, 44.77it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1040439.86it/s]\n",
            "Punc remove: ., label=openweb @ test-dirty.jsonl\n",
            "- True Positive Rate: 89.507%\n",
            "- True Negative Rate: 79.160%\n",
            "- False Positive Rate: 20.840%\n",
            "- False Negative Rate: 10.493%\n",
            "- Accuracy: 81.270%\n",
            "- F1 Score: 66.092%\n",
            "- Recall: 89.507%\n",
            "- Precision: 52.387%\n",
            "- AUROC: 91.773%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ., label=chatgpt @ test-dirty.jsonl\n",
            "- True Positive Rate: 48.030%\n",
            "- True Negative Rate: 99.777%\n",
            "- False Positive Rate: 0.223%\n",
            "- False Negative Rate: 51.970%\n",
            "- Accuracy: 89.198%\n",
            "- F1 Score: 64.514%\n",
            "- Recall: 48.030%\n",
            "- Precision: 98.228%\n",
            "- AUROC: 87.743%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ., label=palm @ test-dirty.jsonl\n",
            "- True Positive Rate: 46.270%\n",
            "- True Negative Rate: 99.161%\n",
            "- False Positive Rate: 0.839%\n",
            "- False Negative Rate: 53.730%\n",
            "- Accuracy: 88.326%\n",
            "- F1 Score: 61.889%\n",
            "- Recall: 46.270%\n",
            "- Precision: 93.424%\n",
            "- AUROC: 88.643%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ., label=llama @ test-dirty.jsonl\n",
            "- True Positive Rate: 97.556%\n",
            "- True Negative Rate: 98.470%\n",
            "- False Positive Rate: 1.530%\n",
            "- False Negative Rate: 2.444%\n",
            "- Accuracy: 98.303%\n",
            "- F1 Score: 95.447%\n",
            "- Recall: 97.556%\n",
            "- Precision: 93.428%\n",
            "- AUROC: 99.344%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ., label=gpt2_xl @ test-dirty.jsonl\n",
            "- True Positive Rate: 98.321%\n",
            "- True Negative Rate: 93.702%\n",
            "- False Positive Rate: 6.298%\n",
            "- False Negative Rate: 1.679%\n",
            "- Accuracy: 94.646%\n",
            "- F1 Score: 88.247%\n",
            "- Recall: 98.321%\n",
            "- Precision: 80.046%\n",
            "- AUROC: 98.588%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:51<00:00, 43.00it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 741544.98it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:28<00:00, 49.57it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1110969.30it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:30<00:00, 49.32it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1028356.29it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:16<00:00, 48.10it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1031969.24it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:46<00:00, 44.42it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 996202.84it/s]\n",
            "Punc remove: ,, label=openweb @ test-dirty.jsonl\n",
            "- True Positive Rate: 89.100%\n",
            "- True Negative Rate: 90.239%\n",
            "- False Positive Rate: 9.761%\n",
            "- False Negative Rate: 10.900%\n",
            "- Accuracy: 90.007%\n",
            "- F1 Score: 78.432%\n",
            "- Recall: 89.100%\n",
            "- Precision: 70.046%\n",
            "- AUROC: 94.629%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ,, label=chatgpt @ test-dirty.jsonl\n",
            "- True Positive Rate: 79.106%\n",
            "- True Negative Rate: 98.827%\n",
            "- False Positive Rate: 1.173%\n",
            "- False Negative Rate: 20.894%\n",
            "- Accuracy: 94.796%\n",
            "- F1 Score: 86.140%\n",
            "- Recall: 79.106%\n",
            "- Precision: 94.546%\n",
            "- AUROC: 95.399%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ,, label=palm @ test-dirty.jsonl\n",
            "- True Positive Rate: 68.189%\n",
            "- True Negative Rate: 99.067%\n",
            "- False Positive Rate: 0.933%\n",
            "- False Negative Rate: 31.811%\n",
            "- Accuracy: 92.742%\n",
            "- F1 Score: 79.377%\n",
            "- Recall: 68.189%\n",
            "- Precision: 94.957%\n",
            "- AUROC: 93.132%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ,, label=llama @ test-dirty.jsonl\n",
            "- True Positive Rate: 96.766%\n",
            "- True Negative Rate: 99.550%\n",
            "- False Positive Rate: 0.450%\n",
            "- False Negative Rate: 3.234%\n",
            "- Accuracy: 99.042%\n",
            "- F1 Score: 97.358%\n",
            "- Recall: 96.766%\n",
            "- Precision: 97.956%\n",
            "- AUROC: 99.295%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ,, label=gpt2_xl @ test-dirty.jsonl\n",
            "- True Positive Rate: 99.594%\n",
            "- True Negative Rate: 95.790%\n",
            "- False Positive Rate: 4.210%\n",
            "- False Negative Rate: 0.406%\n",
            "- Accuracy: 96.567%\n",
            "- F1 Score: 92.226%\n",
            "- Recall: 99.594%\n",
            "- Precision: 85.873%\n",
            "- AUROC: 99.120%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:52<00:00, 42.82it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 1071669.18it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:30<00:00, 49.23it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1181708.19it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:30<00:00, 49.33it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1158517.77it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:16<00:00, 48.11it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1070391.71it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:51<00:00, 43.10it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 946811.40it/s]\n",
            "Punc remove: ?, label=openweb @ test-dirty.jsonl\n",
            "- True Positive Rate: 84.213%\n",
            "- True Negative Rate: 98.686%\n",
            "- False Positive Rate: 1.314%\n",
            "- False Negative Rate: 15.787%\n",
            "- Accuracy: 95.734%\n",
            "- F1 Score: 88.953%\n",
            "- Recall: 84.213%\n",
            "- Precision: 94.257%\n",
            "- AUROC: 96.678%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ?, label=chatgpt @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.475%\n",
            "- True Negative Rate: 98.820%\n",
            "- False Positive Rate: 1.180%\n",
            "- False Negative Rate: 5.525%\n",
            "- Accuracy: 97.932%\n",
            "- F1 Score: 94.919%\n",
            "- Recall: 94.475%\n",
            "- Precision: 95.366%\n",
            "- AUROC: 98.860%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ?, label=palm @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.581%\n",
            "- True Negative Rate: 96.230%\n",
            "- False Positive Rate: 3.770%\n",
            "- False Negative Rate: 5.419%\n",
            "- Accuracy: 95.892%\n",
            "- F1 Score: 90.415%\n",
            "- Recall: 94.581%\n",
            "- Precision: 86.600%\n",
            "- AUROC: 98.420%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ?, label=llama @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.155%\n",
            "- True Negative Rate: 99.936%\n",
            "- False Positive Rate: 0.064%\n",
            "- False Negative Rate: 5.845%\n",
            "- Accuracy: 98.882%\n",
            "- F1 Score: 96.846%\n",
            "- Recall: 94.155%\n",
            "- Precision: 99.695%\n",
            "- AUROC: 98.875%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ?, label=gpt2_xl @ test-dirty.jsonl\n",
            "- True Positive Rate: 98.389%\n",
            "- True Negative Rate: 98.034%\n",
            "- False Positive Rate: 1.966%\n",
            "- False Negative Rate: 1.611%\n",
            "- Accuracy: 98.107%\n",
            "- F1 Score: 95.505%\n",
            "- Recall: 98.389%\n",
            "- Precision: 92.785%\n",
            "- AUROC: 99.529%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:57<00:00, 41.52it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 1076785.53it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:34<00:00, 47.67it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 843635.88it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:30<00:00, 49.03it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1129717.17it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:17<00:00, 48.03it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1112591.84it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:47<00:00, 44.01it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1056624.08it/s]\n",
            "Punc remove: !, label=openweb @ test-dirty.jsonl\n",
            "- True Positive Rate: 83.806%\n",
            "- True Negative Rate: 98.724%\n",
            "- False Positive Rate: 1.276%\n",
            "- False Negative Rate: 16.194%\n",
            "- Accuracy: 95.682%\n",
            "- F1 Score: 88.783%\n",
            "- Recall: 83.806%\n",
            "- Precision: 94.389%\n",
            "- AUROC: 96.598%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: !, label=chatgpt @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.543%\n",
            "- True Negative Rate: 98.786%\n",
            "- False Positive Rate: 1.214%\n",
            "- False Negative Rate: 5.457%\n",
            "- Accuracy: 97.918%\n",
            "- F1 Score: 94.890%\n",
            "- Recall: 94.543%\n",
            "- Precision: 95.239%\n",
            "- AUROC: 98.875%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: !, label=palm @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.649%\n",
            "- True Negative Rate: 96.129%\n",
            "- False Positive Rate: 3.871%\n",
            "- False Negative Rate: 5.351%\n",
            "- Accuracy: 95.825%\n",
            "- F1 Score: 90.281%\n",
            "- Recall: 94.649%\n",
            "- Precision: 86.299%\n",
            "- AUROC: 98.407%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: !, label=llama @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.064%\n",
            "- True Negative Rate: 99.936%\n",
            "- False Positive Rate: 0.064%\n",
            "- False Negative Rate: 5.936%\n",
            "- Accuracy: 98.865%\n",
            "- F1 Score: 96.797%\n",
            "- Recall: 94.064%\n",
            "- Precision: 99.694%\n",
            "- AUROC: 98.814%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: !, label=gpt2_xl @ test-dirty.jsonl\n",
            "- True Positive Rate: 98.348%\n",
            "- True Negative Rate: 98.006%\n",
            "- False Positive Rate: 1.994%\n",
            "- False Negative Rate: 1.652%\n",
            "- Accuracy: 98.076%\n",
            "- F1 Score: 95.434%\n",
            "- Recall: 98.348%\n",
            "- Precision: 92.688%\n",
            "- AUROC: 99.519%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:50<00:00, 43.11it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 1034013.91it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:29<00:00, 49.54it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1193041.45it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:28<00:00, 49.72it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1152623.65it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:16<00:00, 48.36it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1143064.98it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:46<00:00, 44.40it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1002944.41it/s]\n",
            "Punc remove: :, label=openweb @ test-dirty.jsonl\n",
            "- True Positive Rate: 84.268%\n",
            "- True Negative Rate: 98.616%\n",
            "- False Positive Rate: 1.384%\n",
            "- False Negative Rate: 15.732%\n",
            "- Accuracy: 95.690%\n",
            "- F1 Score: 88.857%\n",
            "- Recall: 84.268%\n",
            "- Precision: 93.975%\n",
            "- AUROC: 96.644%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: :, label=chatgpt @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.502%\n",
            "- True Negative Rate: 98.789%\n",
            "- False Positive Rate: 1.211%\n",
            "- False Negative Rate: 5.498%\n",
            "- Accuracy: 97.913%\n",
            "- F1 Score: 94.875%\n",
            "- Recall: 94.502%\n",
            "- Precision: 95.250%\n",
            "- AUROC: 98.857%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: :, label=palm @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.419%\n",
            "- True Negative Rate: 96.320%\n",
            "- False Positive Rate: 3.680%\n",
            "- False Negative Rate: 5.581%\n",
            "- Accuracy: 95.931%\n",
            "- F1 Score: 90.482%\n",
            "- Recall: 94.419%\n",
            "- Precision: 86.860%\n",
            "- AUROC: 98.402%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: :, label=llama @ test-dirty.jsonl\n",
            "- True Positive Rate: 93.563%\n",
            "- True Negative Rate: 99.939%\n",
            "- False Positive Rate: 0.061%\n",
            "- False Negative Rate: 6.437%\n",
            "- Accuracy: 98.776%\n",
            "- F1 Score: 96.538%\n",
            "- Recall: 93.563%\n",
            "- Precision: 99.709%\n",
            "- AUROC: 98.769%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: :, label=gpt2_xl @ test-dirty.jsonl\n",
            "- True Positive Rate: 98.389%\n",
            "- True Negative Rate: 97.867%\n",
            "- False Positive Rate: 2.133%\n",
            "- False Negative Rate: 1.611%\n",
            "- Accuracy: 97.974%\n",
            "- F1 Score: 95.204%\n",
            "- Recall: 98.389%\n",
            "- Precision: 92.220%\n",
            "- AUROC: 99.493%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:50<00:00, 43.08it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 1114577.70it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:28<00:00, 49.60it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 944328.99it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:29<00:00, 49.41it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 751758.41it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:16<00:00, 48.14it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1150730.16it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:46<00:00, 44.29it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 942748.21it/s]\n",
            "Punc remove: ', label=openweb @ test-dirty.jsonl\n",
            "- True Positive Rate: 85.842%\n",
            "- True Negative Rate: 97.834%\n",
            "- False Positive Rate: 2.166%\n",
            "- False Negative Rate: 14.158%\n",
            "- Accuracy: 95.388%\n",
            "- F1 Score: 88.361%\n",
            "- Recall: 85.842%\n",
            "- Precision: 91.032%\n",
            "- AUROC: 96.904%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ', label=chatgpt @ test-dirty.jsonl\n",
            "- True Positive Rate: 93.717%\n",
            "- True Negative Rate: 98.987%\n",
            "- False Positive Rate: 1.013%\n",
            "- False Negative Rate: 6.283%\n",
            "- Accuracy: 97.910%\n",
            "- F1 Score: 94.828%\n",
            "- Recall: 93.717%\n",
            "- Precision: 95.965%\n",
            "- AUROC: 98.790%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ', label=palm @ test-dirty.jsonl\n",
            "- True Positive Rate: 93.824%\n",
            "- True Negative Rate: 96.418%\n",
            "- False Positive Rate: 3.582%\n",
            "- False Negative Rate: 6.176%\n",
            "- Accuracy: 95.886%\n",
            "- F1 Score: 90.333%\n",
            "- Recall: 93.824%\n",
            "- Precision: 87.092%\n",
            "- AUROC: 98.342%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ', label=llama @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.125%\n",
            "- True Negative Rate: 99.926%\n",
            "- False Positive Rate: 0.074%\n",
            "- False Negative Rate: 5.875%\n",
            "- Accuracy: 98.868%\n",
            "- F1 Score: 96.807%\n",
            "- Recall: 94.125%\n",
            "- Precision: 99.646%\n",
            "- AUROC: 98.910%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: ', label=gpt2_xl @ test-dirty.jsonl\n",
            "- True Positive Rate: 96.466%\n",
            "- True Negative Rate: 98.104%\n",
            "- False Positive Rate: 1.896%\n",
            "- False Negative Rate: 3.534%\n",
            "- Accuracy: 97.769%\n",
            "- F1 Score: 94.646%\n",
            "- Recall: 96.466%\n",
            "- Precision: 92.893%\n",
            "- AUROC: 99.443%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:51<00:00, 43.06it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 1063554.11it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:29<00:00, 49.33it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1187780.31it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:29<00:00, 49.48it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1183476.31it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:16<00:00, 48.26it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1083233.89it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:47<00:00, 44.09it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1038695.38it/s]\n",
            "Punc remove: \", label=openweb @ test-dirty.jsonl\n",
            "- True Positive Rate: 83.833%\n",
            "- True Negative Rate: 98.341%\n",
            "- False Positive Rate: 1.659%\n",
            "- False Negative Rate: 16.167%\n",
            "- Accuracy: 95.383%\n",
            "- F1 Score: 88.103%\n",
            "- Recall: 83.833%\n",
            "- Precision: 92.830%\n",
            "- AUROC: 96.570%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: \", label=chatgpt @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.611%\n",
            "- True Negative Rate: 98.674%\n",
            "- False Positive Rate: 1.326%\n",
            "- False Negative Rate: 5.389%\n",
            "- Accuracy: 97.844%\n",
            "- F1 Score: 94.720%\n",
            "- Recall: 94.611%\n",
            "- Precision: 94.829%\n",
            "- AUROC: 98.818%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: \", label=palm @ test-dirty.jsonl\n",
            "- True Positive Rate: 93.959%\n",
            "- True Negative Rate: 96.247%\n",
            "- False Positive Rate: 3.753%\n",
            "- False Negative Rate: 6.041%\n",
            "- Accuracy: 95.778%\n",
            "- F1 Score: 90.117%\n",
            "- Recall: 93.959%\n",
            "- Precision: 86.577%\n",
            "- AUROC: 98.288%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: \", label=llama @ test-dirty.jsonl\n",
            "- True Positive Rate: 92.713%\n",
            "- True Negative Rate: 99.936%\n",
            "- False Positive Rate: 0.064%\n",
            "- False Negative Rate: 7.287%\n",
            "- Accuracy: 98.619%\n",
            "- F1 Score: 96.075%\n",
            "- Recall: 92.713%\n",
            "- Precision: 99.690%\n",
            "- AUROC: 98.463%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: \", label=gpt2_xl @ test-dirty.jsonl\n",
            "- True Positive Rate: 97.969%\n",
            "- True Negative Rate: 97.902%\n",
            "- False Positive Rate: 2.098%\n",
            "- False Negative Rate: 2.031%\n",
            "- Accuracy: 97.916%\n",
            "- F1 Score: 95.054%\n",
            "- Recall: 97.969%\n",
            "- Precision: 92.307%\n",
            "- AUROC: 99.460%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-web-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7367/7367 [02:52<00:00, 42.69it/s]\n",
            "Reducing: 100% 7367/7367 [00:00<00:00, 684039.62it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-gpt-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:30<00:00, 49.08it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1114647.34it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-palm-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7400/7400 [02:29<00:00, 49.59it/s]\n",
            "Reducing: 100% 7400/7400 [00:00<00:00, 1168022.04it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/open-llama-text/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 6587/6587 [02:17<00:00, 47.97it/s]\n",
            "Reducing: 100% 6587/6587 [00:00<00:00, 1050689.50it/s]\n",
            "Cache Miss / Eviction since argument does not match\n",
            "PipelineExecutor: mapreduce sequentially\n",
            "\t[data/split/gpt2-output/test-dirty.jsonl] --[Map FromJsonStr >> RemoveSingleton >> ExecuteT5 >> T5PredictToLogits >> ToSingletonList]--> [Reduce <function list_reduce at 0x7bb2614305e0>] -->\n",
            "Mapping: 100% 7385/7385 [02:45<00:00, 44.50it/s]\n",
            "Reducing: 100% 7385/7385 [00:00<00:00, 1080395.36it/s]\n",
            "Punc remove: *, label=openweb @ test-dirty.jsonl\n",
            "- True Positive Rate: 83.304%\n",
            "- True Negative Rate: 98.512%\n",
            "- False Positive Rate: 1.488%\n",
            "- False Negative Rate: 16.696%\n",
            "- Accuracy: 95.410%\n",
            "- F1 Score: 88.099%\n",
            "- Recall: 83.304%\n",
            "- Precision: 93.481%\n",
            "- AUROC: 96.421%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: *, label=chatgpt @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.611%\n",
            "- True Negative Rate: 98.629%\n",
            "- False Positive Rate: 1.371%\n",
            "- False Negative Rate: 5.389%\n",
            "- Accuracy: 97.808%\n",
            "- F1 Score: 94.636%\n",
            "- Recall: 94.611%\n",
            "- Precision: 94.662%\n",
            "- AUROC: 98.819%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: *, label=palm @ test-dirty.jsonl\n",
            "- True Positive Rate: 92.824%\n",
            "- True Negative Rate: 96.010%\n",
            "- False Positive Rate: 3.990%\n",
            "- False Negative Rate: 7.176%\n",
            "- Accuracy: 95.358%\n",
            "- F1 Score: 89.121%\n",
            "- Recall: 92.824%\n",
            "- Precision: 85.702%\n",
            "- AUROC: 97.808%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: *, label=llama @ test-dirty.jsonl\n",
            "- True Positive Rate: 94.155%\n",
            "- True Negative Rate: 99.932%\n",
            "- False Positive Rate: 0.068%\n",
            "- False Negative Rate: 5.845%\n",
            "- Accuracy: 98.879%\n",
            "- F1 Score: 96.838%\n",
            "- Recall: 94.155%\n",
            "- Precision: 99.679%\n",
            "- AUROC: 98.890%\n",
            "\n",
            "\n",
            "Executing task:  punctuation_removal\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Reusing existing cache from cache/t5_punctuation_removal.pt\n",
            "Punc remove: *, label=gpt2_xl @ test-dirty.jsonl\n",
            "- True Positive Rate: 98.321%\n",
            "- True Negative Rate: 97.944%\n",
            "- False Positive Rate: 2.056%\n",
            "- False Negative Rate: 1.679%\n",
            "- Accuracy: 98.021%\n",
            "- F1 Score: 95.307%\n",
            "- Recall: 98.321%\n",
            "- Precision: 92.473%\n",
            "- AUROC: 99.509%\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "openweb-to-rest @ test.variant4.jsonl\n",
            "TPR: 0.885,\tTNR: 0.957, FPR: 0.043,\tFNR: 0.115 \n",
            "Acc: 0.943\n",
            "F1 : 0.863\n",
            "Recall: 0.885,\tPrecision: 0.842\n",
            "AUROC: 0.966\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "chatgpt-to-rest @ test.variant4.jsonl\n",
            "TPR: 0.889,\tTNR: 0.993, FPR: 0.007,\tFNR: 0.111 \n",
            "Acc: 0.972\n",
            "F1 : 0.928\n",
            "Recall: 0.889,\tPrecision: 0.972\n",
            "AUROC: 0.984\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "palm-to-rest @ test.variant4.jsonl\n",
            "TPR: 0.86,\tTNR: 0.981, FPR: 0.019,\tFNR: 0.14 \n",
            "Acc: 0.956\n",
            "F1 : 0.889\n",
            "Recall: 0.86,\tPrecision: 0.921\n",
            "AUROC: 0.973\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "llama-to-rest @ test.variant4.jsonl\n",
            "TPR: 0.932,\tTNR: 0.999, FPR: 0.001,\tFNR: 0.068 \n",
            "Acc: 0.986\n",
            "F1 : 0.962\n",
            "Recall: 0.932,\tPrecision: 0.994\n",
            "AUROC: 0.987\n",
            "\n",
            "\n",
            "Executing task:  implementation\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "T5-Sentinel (ours) classifier statistics\n",
            "gpt2_xl-to-rest @ test.variant4.jsonl\n",
            "TPR: 0.974,\tTNR: 0.959, FPR: 0.041,\tFNR: 0.026 \n",
            "Acc: 0.962\n",
            "F1 : 0.914\n",
            "Recall: 0.974,\tPrecision: 0.86\n",
            "AUROC: 0.989\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 evaluator/calc/calc_accuracy.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDyJgb1Isfgf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4HOXT-AP1XU",
        "outputId": "80f9c900-426f-413f-a6bc-abe522ad218c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing task:  sample_pca_t5_full_edge\n",
            "Reusing existing cache from cache/t5_full_hiddens.pt\n",
            "Reusing existing cache from cache/t5_full_hiddens.pt\n",
            "Reusing existing cache from cache/t5_full_hiddens.pt\n",
            "Reusing existing cache from cache/t5_full_hiddens.pt\n",
            "Reusing existing cache from cache/t5_full_hiddens.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n"
          ]
        }
      ],
      "source": [
        "!python3 evaluator/interpret/sample_pca.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf9XwwyZsgMJ",
        "outputId": "fe668f41-6ff7-4b6f-9795-26151c0d1ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing task:  plot_full_confusion_mat\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Executing task:  implement\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Executing task:  implement\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Executing task:  implement\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Executing task:  implement\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Executing task:  plot_full_confusion_mat_compare\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n",
            "Reusing existing cache from cache/t5_full_predicts.pt\n"
          ]
        }
      ],
      "source": [
        "!python3 evaluator/plot/*.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfSgL4GBz8xb",
        "outputId": "f9bb9486-bf60-4c42-ae83-af47b9ebcbd1"
      },
      "outputs": [],
      "source": [
        "!python3 detector/t5_hidden/__main__.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
